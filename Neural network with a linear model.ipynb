{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Students:\n",
    "\n",
    "- ...\n",
    "- ...\n",
    "- ...\n",
    "\n",
    "# Practical classes\n",
    "\n",
    "\n",
    "All exercices will be in Python. It is important that you keep track of exercices and structure you code correctly (e.g. create funcions that you can re-use later)\n",
    "\n",
    "We will use Jupyter notebooks (formerly known as IPython). You can read the following courses for help:\n",
    "* Python and numpy: http://cs231n.github.io/python-numpy-tutorial/\n",
    "* Jupyter / IPython : http://cs231n.github.io/ipython-tutorial/\n",
    "\n",
    "\n",
    "# Neural network: first experiments with a linear model\n",
    "\n",
    "In this first lab exercise we will code a neural network using numpy, without a neural network library.\n",
    "Next week, the lab exercise will be to extend this program with hidden layers and activation functions.\n",
    "\n",
    "The task is digit recognition: the neural network has to predict which digit in $\\{0...9\\}$ is written in the input picture. We will use the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, a standard benchmark in machine learning.\n",
    "\n",
    "The model is a simple linear  classifier $o = \\operatorname{softmax}(Wx + b)$ where:\n",
    "* $x$ is an input image that is represented as a column vector, each value being the \"color\" of a pixel\n",
    "* $W$ and $b$ are the parameters of the classifier\n",
    "* $\\operatorname{softmax}$ transforms the output weight (logits) into probabilities\n",
    "* $o$ is column vector that contains the probability of each category\n",
    "\n",
    "We will train this model via stochastic gradient descent by minimizing the negative log-likelihood of the data:\n",
    "$$\n",
    "    \\hat{W}, \\hat{b} = \\operatorname{argmin}_{W, b} \\sum_{x, y} - \\log p(y | x)\n",
    "$$\n",
    "Although this is a linear model, it classifies raw data without any manual feature extraction step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libs that we will use\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# To load the data we will use the script of Gaetan Marceau Caron\n",
    "# You can download it from the course webiste and move it to the same directory that contains this ipynb file\n",
    "import dataset_loader\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mnist dataset \n",
    "# if(\"mnist.pkl.gz\" not in os.listdir(\".\")):\n",
    "    # this link doesn't work any more,\n",
    "    # seach on google for the file \"mnist.pkl.gz\"\n",
    "    # and download it\n",
    "    # https://github.com/MichalDanielDobrzanski/DeepLearningPython/blob/master/mnist.pkl.gz\n",
    "\n",
    "# if you have it somewhere else, you can comment the lines above\n",
    "# and overwrite the path below\n",
    "mnist_path = \"./mnist.pkl.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 3 splits\n",
    "train_data, dev_data, test_data = dataset_loader.load_mnist(mnist_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is a list with two elemets:\n",
    "* data[0] contains images\n",
    "* data[1] contains labels\n",
    "\n",
    "Data is stored as numpy.ndarray. You can use data[0][i] to retrieve image number i and data[1][i] to retrieve its label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.int64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data))\n",
    "print(type(train_data[0]))\n",
    "print(type(train_data[1]))\n",
    "print(type(train_data[0][0]))\n",
    "print(type(train_data[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a0a1a10910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 900\n",
    "label = train_data[1][index]\n",
    "picture = train_data[0][index]\n",
    "\n",
    "print(\"label: %i\" % label)\n",
    "plt.imshow(picture.reshape(28,28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What are the characteristics of training data? (number of samples, dimension of input, number of labels)\n",
    "\n",
    "The documentation of ndarray class is available here: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDimDataset(data):\n",
    "    n_training = data[0].shape[0]\n",
    "    n_feature = data[0].shape[1]\n",
    "    n_label = len(set(data[1][i] for i in range(len(data[1]))))\n",
    "    return n_training, n_feature, n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getDimDataset(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Building functions\n",
    "\n",
    "We now need to build functions that are required for the neural network.\n",
    "$$\n",
    "    o = \\operatorname{softmax}(Wx + b) \\\\\n",
    "    L(x, y) = -\\log p(y | x) = -\\log o[y]\n",
    "$$\n",
    "\n",
    "Note that in numpy, operator @ is used for matrix multiplication while * is used for element-wise multiplication.\n",
    "The documentation for linear algebra in numpy is available here: https://docs.scipy.org/doc/numpy/reference/routines.linalg.html\n",
    "\n",
    "The first operation is the affine transformation $v = Wx + b$.\n",
    "To compute the gradient, it is often convenient to write the forward pass as $v[i] = b[i] + \\sum_j W[i, j] x[j]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# Output:\n",
    "# - vector\n",
    "def affine_transform(W, b, x):\n",
    "    v= W.dot(x) + b\n",
    "    return v\n",
    "\n",
    "# Input:\n",
    "# - W: projection matrix\n",
    "# - b: bias\n",
    "# - x: input features\n",
    "# - g: incoming gradient\n",
    "# Output:\n",
    "# - g_W: gradient wrt W\n",
    "# - g_b: gradient wrt b\n",
    "def backward_affine_transform(W, b, x, g):\n",
    "    g_W = np.outer(g, x)\n",
    "    g_b = g\n",
    "    return g_W, g_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is a (too simple) test of affine_transform and backward_affine_transform.\n",
    "It should run without error if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.asarray([[ 0.63024213,  0.53679375, -0.92079597],\n",
    " [-0.1155045,   0.62780356, -0.67961305],\n",
    " [ 0.08465286, -0.06561815, -0.39778322],\n",
    " [ 0.8242268,   0.58907262, -0.52208052],\n",
    " [-0.43894227, -0.56993247,  0.09520727]])\n",
    "b = np.asarray([ 0.42706842,  0.69636598, -0.85611933, -0.08682553,  0.83160079])\n",
    "x = np.asarray([-0.32809223, -0.54751413,  0.81949319])\n",
    "\n",
    "o_gold = np.asarray([-0.82819732, -0.16640748, -1.17394705, -1.10761496,  1.36568213])\n",
    "g = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "g_W_gold = np.asarray([[ 0.02932773,  0.04894156, -0.07325341],\n",
    " [-0.14463576, -0.24136543,  0.36126434],\n",
    " [ 0.07417322,  0.12377887, -0.18526635],\n",
    " [ 0.31561399,  0.52669067, -0.78832562],\n",
    " [ 0.17529576,  0.29253025, -0.43784542]])\n",
    "g_b_gold = np.asarray([-0.08938868,  0.44083873, -0.2260743,  -0.96196726, -0.53428805])\n",
    "\n",
    "\n",
    "# quick test of the forward pass\n",
    "o = affine_transform(W, b, x)\n",
    "if o.shape != o_gold.shape:\n",
    "    raise RuntimeError(\"Unexpected output dimension: got %s, expected %s\" % (str(o.shape), str(o_gold.shape)))\n",
    "if not np.allclose(o, o_gold):\n",
    "    raise RuntimeError(\"Output of the affine_transform function is incorrect\")\n",
    "    \n",
    "# quick test if the backward pass\n",
    "g_W, g_b = backward_affine_transform(W, b, x, g)\n",
    "if g_W.shape != g_W_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for W: got %s, expected %s\" % (str(g_W.shape), str(g_W_gold.shape)))\n",
    "if g_b.shape != g_b_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension for b: got %s, expected %s\" % (str(g_b.shape), str(g_b_gold.shape)))\n",
    "if not np.allclose(g_W, g_W_gold):\n",
    "    raise RuntimeError(\"Gradient of W is incorrect\")\n",
    "if not np.allclose(g_b, g_b_gold):\n",
    "    raise RuntimeError(\"Gradient of b is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function:\n",
    "$$\n",
    "     o = \\operatorname{softmax}(w)\n",
    "$$\n",
    "where $w$ is a vector of logits in $\\mathbb R$ and $o$ a vector of probabilities such that:\n",
    "$$\n",
    "    o[i] = \\frac{\\exp(w[i])}{\\sum_j \\exp(w[j])}\n",
    "$$\n",
    "We do not need to implement the backward for this experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# Output\n",
    "# - vector of probabilities\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING:** is your implementation numerically stable?\n",
    "\n",
    "The $\\exp$ function results in computations that overflows (i.e. results in numbers that cannot be represented with floating point numbers).\n",
    "Therefore, it is always convenient to use the following trick to improve stability: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Example for testing the numerical stability of softmax\n",
    "# It should return [1., 0. ,0.], not [nan, 0., 0.]\n",
    "z = [1000000,1,100]\n",
    "print(softmax(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: from the result of the cell above, what can you say about the softmax output, even when it is stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just too simple test for the softmax function\n",
    "x = np.asarray([0.92424884, -0.92381088, -0.74666024, -0.87705478, -0.54797015])\n",
    "y_gold = np.asarray([0.57467369, 0.09053556, 0.10808233, 0.09486917, 0.13183925])\n",
    "\n",
    "y = softmax(x)\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output of the softmax function is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we build the loss function and its gradient for training the network.\n",
    "\n",
    "The loss function is the negative log-likelihood defined as:\n",
    "$$\n",
    "    \\mathcal L(x, gold) = -\\log \\frac{\\exp(x[gold])}{\\sum_j \\exp(x[j])} = -x[gold] + \\log \\sum_j \\exp(x[j])\n",
    "$$\n",
    "This function is also called the cross-entropy loss (in Pytorch, different names are used dependending if the inputs are probabilities or raw logits).\n",
    "\n",
    "Similarly to the softmax, we have to rely on the log-sum-exp trick to stabilize the computation: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# Output:\n",
    "# - scalare equal to -log(softmax(x)[gold])\n",
    "def nll(x, gold):\n",
    "    return -np.log(softmax(x)[gold])\n",
    "\n",
    "# Input:\n",
    "# - x: vector of logits\n",
    "# - gold: index of the gold class\n",
    "# - gradient (scalar)\n",
    "# Output:\n",
    "# - gradient wrt x\n",
    "def backward_nll(x, gold, g):\n",
    "    g_x = softmax(x)\n",
    "    y = np.zeros(x.shape)\n",
    "    y[gold] = -1\n",
    "    g_x = g_x + y\n",
    "    g_x = g * g_x\n",
    "    return g_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x = np.asarray([-0.13590009, -0.83649656,  0.03130881,  0.42559402,  0.08488182])\n",
    "y_gold = 1.5695014420179738\n",
    "g_gold = np.asarray([ 0.17609875,  0.08739591, -0.79185107,  0.30875221,  0.2196042 ])\n",
    "\n",
    "y = nll(x, 2)\n",
    "g = backward_nll(x, 2, 1.)\n",
    "\n",
    "if not np.allclose(y, y_gold):\n",
    "    raise RuntimeError(\"Output is incorrect\")\n",
    "\n",
    "if g.shape != g_gold.shape:\n",
    "        raise RuntimeError(\"Unexpected gradient dimension: got %s, expected %s\" % (str(g.shape), str(g_gold.shape)))\n",
    "if not np.allclose(g, g_gold):\n",
    "    raise RuntimeError(\"Gradient is incorrect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code test the implementation of the gradient using finite-difference approximation, see: https://timvieira.github.io/blog/post/2017/04/21/how-to-test-gradient-implementations/\n",
    "\n",
    "Your implementation should pass this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is python re-implementation of the test from the Dynet library\n",
    "# https://github.com/clab/dynet/blob/master/dynet/grad-check.cc\n",
    "\n",
    "def is_almost_equal(grad, computed_grad):\n",
    "    #print(grad, computed_grad)\n",
    "    f = abs(grad - computed_grad)\n",
    "    m = max(abs(grad), abs(computed_grad))\n",
    "\n",
    "    if f > 0.01 and m > 0.:\n",
    "        f /= m\n",
    "\n",
    "    if f > 0.01 or math.isnan(f):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def check_gradient(function, weights, true_grad, alpha = 1e-3):\n",
    "    # because input can be of any dimension,\n",
    "    # we build a view of the underlying data with the .shape(-1) method\n",
    "    # then we can access any element of the tensor as a elements of a list\n",
    "    # with a single dimension\n",
    "    weights_view = weights.reshape(-1)\n",
    "    true_grad_view = true_grad.reshape(-1)\n",
    "    for i in range(weights_view.shape[0]):\n",
    "        old = weights_view[i]\n",
    "\n",
    "        weights_view[i] = old - alpha\n",
    "        value_left = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old + alpha\n",
    "        value_right = function(weights).reshape(-1)\n",
    "\n",
    "        weights_view[i] = old\n",
    "        grad = (value_right - value_left) / (2. * alpha)\n",
    "\n",
    "        if not is_almost_equal(grad, true_grad_view[i]):\n",
    "            return False\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test the affine transformation\n",
    "\n",
    "x = np.random.uniform(-1, 1, (5,))\n",
    "W = np.random.uniform(-1, 1, (3, 5))\n",
    "b = np.random.uniform(-1, 1, (3,))\n",
    "\n",
    "for i in range(3):\n",
    "    y = affine_transform(W, b, x)\n",
    "    g = np.zeros_like(y)\n",
    "    g[i] = 1.\n",
    "    g_W, _ = backward_affine_transform(W, b, x, g)\n",
    "    print(check_gradient(lambda W: affine_transform(W, b, x)[i], W, g_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# test the negative likelihood loss\n",
    "\n",
    "x = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "for gold in range(5):\n",
    "    y = nll(x, gold)\n",
    "    g_y = backward_nll(x, gold, 1.)\n",
    "\n",
    "    print(check_gradient(lambda x: nll(x, gold), x, g_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Parameter initialization\n",
    "\n",
    "We are now going to build the function that will be used to initialize the parameters of the neural network before training.\n",
    "Note that for parameter initialization you must use **in-place** operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random ndarray\n",
    "a = np.random.uniform(-1, 1, (5,))\n",
    "\n",
    "# this does not change the data of the ndarray created above!\n",
    "# it creates a new ndarray and replace the reference stored in a\n",
    "a = np.zeros((5, ))\n",
    "\n",
    "# this will change the underlying data of the ndarray that a points to\n",
    "a[:] = 0\n",
    "\n",
    "# similarly, this creates a new array and change the object pointed by a\n",
    "a = a + 1\n",
    "\n",
    "# while this change the underlying data of a\n",
    "a += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an affine transformation, it is common to:\n",
    "* initialize the bias to 0\n",
    "* initialize the projection matrix with Glorot initialization (also known as Xavier initialization)\n",
    "\n",
    "The formula for Glorot initialization can be found in equation 16 (page 5) of the original paper: http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(b):\n",
    "    b[:] = 0.\n",
    "\n",
    "def glorot_init(W):\n",
    "    n_in, n_out = W.shape\n",
    "    range = np.sqrt(6 / (n_in + n_out))\n",
    "    W[:] = np.random.uniform(low=-range, high=range, size=(n_in, n_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Building and training the neural network\n",
    "\n",
    "In our simple example, creating the neural network is simply instantiating the parameters $W$ and $b$.\n",
    "They must be ndarray object with the correct dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parameters(dim_input, dim_output):\n",
    "    W = np.zeros((dim_output, dim_input))\n",
    "    b = np.zeros((dim_output,))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recent success of deep learning is (partly) due to the ability to train very big neural networks.\n",
    "However, researchers became interested in building small neural networks to improve computational efficiency and memory usage.\n",
    "Therefore, we often want to compare neural networks by their number of parameters, i.e. the size of the memory required to store the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_n_parameters(W, b):\n",
    "    n = n = W.size + b.size\n",
    "    print(\"Number of parameters: %i\" % (n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the neural network and print its number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7850\n"
     ]
    }
   ],
   "source": [
    "dim_input = train_data[0].shape[1]\n",
    "dim_output = len(set(train_data[1]))\n",
    "W, b = create_parameters(dim_input, dim_output)\n",
    "print_n_parameters(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training loop!\n",
    "\n",
    "The training loop should be structured as follows:\n",
    "* we do **epochs** over the data, i.e. one epoch is one loop over the dataset\n",
    "* at each epoch, we first loop over the data and update the network parameters with respect to the loss gradient\n",
    "* at the end of each epoch, we evaluate the network on the dev dataset\n",
    "* after all epochs are done, we evaluate our network on the test dataset and compare its performance with the performance on dev\n",
    "\n",
    "During training, it is useful to print the following information:\n",
    "* the mean loss over the epoch: it should be decreasing!\n",
    "* the accuracy on the dev set: it should be increasing!\n",
    "* the accuracy on the train set: it shoud be increasing!\n",
    "\n",
    "If you observe a decreasing loss (+increasing accuracy on test data) but decreasing accuracy on dev data, your network is overfitting!\n",
    "\n",
    "Once you have build **and tested** this a simple training loop, you should introduce the following improvements:\n",
    "* instead of evaluating on dev after each loop on the training data, you can also evaluate on dev n times per epoch\n",
    "* shuffle the data before each epoch\n",
    "* instead of memorizing the parameters of the last epoch only, you should have a copy of the parameters that produced the best value on dev data during training and evaluate on test with those instead of the parameters after the last epoch\n",
    "* learning rate decay: if you do not observe improvement on dev, you can try to reduce the step size\n",
    "\n",
    "After you conducted (successful?) experiments, you should write a report with results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train_loss=0.2562, train_accuracy=0.9279\n",
      "Epoch 2: train_loss=0.2550, train_accuracy=0.9284\n",
      "Epoch 3: train_loss=0.2554, train_accuracy=0.9289\n",
      "Epoch 4: train_loss=0.2546, train_accuracy=0.9283\n",
      "Epoch 5: train_loss=0.2539, train_accuracy=0.9286\n",
      "Epoch 6: train_loss=0.2537, train_accuracy=0.9280\n",
      "Epoch 7: train_loss=0.2535, train_accuracy=0.9275\n",
      "Epoch 8: train_loss=0.2534, train_accuracy=0.9274\n",
      "Epoch 9: train_loss=0.2528, train_accuracy=0.9281\n",
      "dev_accuracy=0.9277\n",
      "Epoch 10: train_loss=0.2535, train_accuracy=0.9284\n",
      "Epoch 11: train_loss=0.2523, train_accuracy=0.9287\n",
      "Epoch 12: train_loss=0.2522, train_accuracy=0.9289\n",
      "Epoch 13: train_loss=0.2519, train_accuracy=0.9285\n",
      "Epoch 14: train_loss=0.2514, train_accuracy=0.9300\n",
      "Epoch 15: train_loss=0.2509, train_accuracy=0.9291\n",
      "Epoch 16: train_loss=0.2510, train_accuracy=0.9288\n",
      "Epoch 17: train_loss=0.2502, train_accuracy=0.9302\n",
      "Epoch 18: train_loss=0.2502, train_accuracy=0.9293\n",
      "Epoch 19: train_loss=0.2499, train_accuracy=0.9288\n",
      "dev_accuracy=0.9251\n",
      "Epoch 20: train_loss=0.2498, train_accuracy=0.9290\n",
      "Epoch 21: train_loss=0.2502, train_accuracy=0.9301\n",
      "Epoch 22: train_loss=0.2509, train_accuracy=0.9287\n",
      "Epoch 23: train_loss=0.2494, train_accuracy=0.9301\n",
      "Epoch 24: train_loss=0.2501, train_accuracy=0.9288\n",
      "Epoch 25: train_loss=0.2490, train_accuracy=0.9298\n",
      "Epoch 26: train_loss=0.2491, train_accuracy=0.9302\n",
      "Epoch 27: train_loss=0.2491, train_accuracy=0.9292\n",
      "Epoch 28: train_loss=0.2488, train_accuracy=0.9296\n",
      "Epoch 29: train_loss=0.2481, train_accuracy=0.9299\n",
      "dev_accuracy=0.9228\n",
      "Epoch 30: train_loss=0.2481, train_accuracy=0.9296\n",
      "Epoch 31: train_loss=0.2480, train_accuracy=0.9295\n",
      "Epoch 32: train_loss=0.2483, train_accuracy=0.9293\n",
      "Epoch 33: train_loss=0.2480, train_accuracy=0.9304\n",
      "Epoch 34: train_loss=0.2476, train_accuracy=0.9299\n",
      "Epoch 35: train_loss=0.2475, train_accuracy=0.9296\n",
      "Epoch 36: train_loss=0.2466, train_accuracy=0.9305\n",
      "Epoch 37: train_loss=0.2465, train_accuracy=0.9306\n",
      "Epoch 38: train_loss=0.2472, train_accuracy=0.9299\n",
      "Epoch 39: train_loss=0.2468, train_accuracy=0.9304\n",
      "dev_accuracy=0.9260\n",
      "Epoch 40: train_loss=0.2469, train_accuracy=0.9295\n",
      "Epoch 41: train_loss=0.2467, train_accuracy=0.9305\n",
      "Epoch 42: train_loss=0.2469, train_accuracy=0.9300\n",
      "Epoch 43: train_loss=0.2466, train_accuracy=0.9301\n",
      "Epoch 44: train_loss=0.2463, train_accuracy=0.9305\n",
      "Epoch 45: train_loss=0.2457, train_accuracy=0.9311\n",
      "Epoch 46: train_loss=0.2460, train_accuracy=0.9303\n",
      "Epoch 47: train_loss=0.2465, train_accuracy=0.9306\n",
      "Epoch 48: train_loss=0.2454, train_accuracy=0.9301\n",
      "Epoch 49: train_loss=0.2448, train_accuracy=0.9304\n",
      "dev_accuracy=0.9230\n",
      "Epoch 50: train_loss=0.2443, train_accuracy=0.9310\n",
      "Epoch 51: train_loss=0.2454, train_accuracy=0.9304\n",
      "Epoch 52: train_loss=0.2452, train_accuracy=0.9309\n",
      "Epoch 53: train_loss=0.2452, train_accuracy=0.9300\n",
      "Epoch 54: train_loss=0.2445, train_accuracy=0.9313\n",
      "Epoch 55: train_loss=0.2447, train_accuracy=0.9308\n",
      "Epoch 56: train_loss=0.2447, train_accuracy=0.9304\n",
      "Epoch 57: train_loss=0.2443, train_accuracy=0.9306\n",
      "Epoch 58: train_loss=0.2443, train_accuracy=0.9310\n",
      "Epoch 59: train_loss=0.2445, train_accuracy=0.9308\n",
      "dev_accuracy=0.9222\n",
      "Epoch 60: train_loss=0.2445, train_accuracy=0.9314\n",
      "Epoch 61: train_loss=0.2433, train_accuracy=0.9322\n",
      "Epoch 62: train_loss=0.2433, train_accuracy=0.9315\n",
      "Epoch 63: train_loss=0.2435, train_accuracy=0.9311\n",
      "Epoch 64: train_loss=0.2441, train_accuracy=0.9302\n",
      "Epoch 65: train_loss=0.2431, train_accuracy=0.9312\n",
      "Epoch 66: train_loss=0.2435, train_accuracy=0.9310\n",
      "Epoch 67: train_loss=0.2435, train_accuracy=0.9308\n",
      "Epoch 68: train_loss=0.2442, train_accuracy=0.9306\n",
      "Epoch 69: train_loss=0.2438, train_accuracy=0.9310\n",
      "dev_accuracy=0.9232\n",
      "Epoch 70: train_loss=0.2436, train_accuracy=0.9302\n",
      "Epoch 71: train_loss=0.2430, train_accuracy=0.9311\n",
      "Epoch 72: train_loss=0.2433, train_accuracy=0.9318\n",
      "Epoch 73: train_loss=0.2434, train_accuracy=0.9313\n",
      "Epoch 74: train_loss=0.2424, train_accuracy=0.9310\n",
      "Epoch 75: train_loss=0.2426, train_accuracy=0.9308\n",
      "Epoch 76: train_loss=0.2428, train_accuracy=0.9303\n",
      "Epoch 77: train_loss=0.2423, train_accuracy=0.9313\n",
      "Epoch 78: train_loss=0.2421, train_accuracy=0.9312\n",
      "Epoch 79: train_loss=0.2424, train_accuracy=0.9312\n",
      "dev_accuracy=0.9219\n",
      "Epoch 80: train_loss=0.2430, train_accuracy=0.9309\n",
      "Epoch 81: train_loss=0.2432, train_accuracy=0.9307\n",
      "Epoch 82: train_loss=0.2419, train_accuracy=0.9311\n",
      "Epoch 83: train_loss=0.2421, train_accuracy=0.9309\n",
      "Epoch 84: train_loss=0.2419, train_accuracy=0.9314\n",
      "Epoch 85: train_loss=0.2416, train_accuracy=0.9319\n",
      "Epoch 86: train_loss=0.2415, train_accuracy=0.9320\n",
      "Epoch 87: train_loss=0.2424, train_accuracy=0.9310\n",
      "Epoch 88: train_loss=0.2419, train_accuracy=0.9307\n",
      "Epoch 89: train_loss=0.2408, train_accuracy=0.9309\n",
      "dev_accuracy=0.9247\n",
      "Epoch 90: train_loss=0.2421, train_accuracy=0.9315\n",
      "Epoch 91: train_loss=0.2410, train_accuracy=0.9318\n",
      "Epoch 92: train_loss=0.2424, train_accuracy=0.9321\n",
      "Epoch 93: train_loss=0.2407, train_accuracy=0.9317\n",
      "Epoch 94: train_loss=0.2414, train_accuracy=0.9316\n",
      "Epoch 95: train_loss=0.2419, train_accuracy=0.9317\n",
      "Epoch 96: train_loss=0.2417, train_accuracy=0.9307\n",
      "Epoch 97: train_loss=0.2414, train_accuracy=0.9318\n",
      "Epoch 98: train_loss=0.2416, train_accuracy=0.9319\n",
      "Epoch 99: train_loss=0.2415, train_accuracy=0.9314\n",
      "dev_accuracy=0.9184\n",
      "Epoch 100: train_loss=0.2402, train_accuracy=0.9328\n",
      "Best parameters: dev_accuracy=0.9277, test_accuracy=0.9245\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100  # number of epochs\n",
    "step = 0.01  # step size for gradient updates\n",
    "best_dev_accuracy = 0.0\n",
    "best_params = (W.copy(), b.copy())\n",
    "n_training = train_data[0].shape[0]\n",
    "n_dev = dev_data[0].shape[0]\n",
    "n_test = test_data[0].shape[0]\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # shuffle the data before each epoch\n",
    "    indices = np.random.permutation(n_training)\n",
    "    train_data_shuffled = (train_data[0][indices], train_data[1][indices])\n",
    "\n",
    "    # train the model on the training data\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    for x, y in zip(train_data_shuffled[0], train_data_shuffled[1]):\n",
    "        # forward pass\n",
    "        y_hat = affine_transform(W, b, x)\n",
    "        loss = nll(y_hat, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # backward pass\n",
    "        g_y_hat = backward_nll(y_hat, y, 1.)\n",
    "        g_W, g_b = backward_affine_transform(W, b, x, g_y_hat)\n",
    "\n",
    "        # gradient update\n",
    "        W -= step * g_W\n",
    "        b -= step * g_b\n",
    "\n",
    "        # compute accuracy on train set\n",
    "        train_correct += int(np.argmax(y_hat) == y)\n",
    "\n",
    "    train_accuracy = train_correct / n_training\n",
    "    train_loss /= n_training\n",
    "\n",
    "    # evaluate the model on the dev data\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        dev_correct = 0\n",
    "        for x, y in zip(dev_data[0], dev_data[1]):\n",
    "            y_hat = affine_transform(W, b, x)\n",
    "            dev_correct += int(np.argmax(y_hat) == y)\n",
    "\n",
    "        dev_accuracy = dev_correct / n_dev\n",
    "\n",
    "        # check if dev accuracy has improved and update best parameters\n",
    "        if dev_accuracy > best_dev_accuracy:\n",
    "            best_dev_accuracy = dev_accuracy\n",
    "            best_params = (W.copy(), b.copy())\n",
    "        \n",
    "        print(\"dev_accuracy=%.4f\" %(dev_accuracy))\n",
    "\n",
    "    # print progress\n",
    "    print(\"Epoch %d: train_loss=%.4f, train_accuracy=%.4f\" %\n",
    "          (epoch+1, train_loss, train_accuracy))\n",
    "\n",
    "# evaluate the model on the test data with the best parameters\n",
    "W_best, b_best = best_params\n",
    "test_correct = 0\n",
    "for x, y in zip(test_data[0], test_data[1]):\n",
    "    y_hat = affine_transform(W_best, b_best, x)\n",
    "    test_correct += int(np.argmax(y_hat) == y)\n",
    "\n",
    "test_accuracy = test_correct / n_test\n",
    "\n",
    "print(\"Best parameters: dev_accuracy=%.4f, test_accuracy=%.4f\" % (best_dev_accuracy, test_accuracy))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
